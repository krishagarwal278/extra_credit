{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SuQuGti_OJm8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ga1g3p6oOJm9"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_path = 'train.csv'\n",
        "test_path = 'test.csv'\n",
        "sample_submission_path = 'sample_submission.csv'\n",
        "\n",
        "# Read data\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "sample_submission = pd.read_csv(sample_submission_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XW3ZC-_ZOJm9"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "def preprocess_and_engineer_features(df):\n",
        "    # Date and time features\n",
        "    df['trans_date'] = pd.to_datetime(df['trans_date'])\n",
        "    df['trans_year'] = df['trans_date'].dt.year\n",
        "    df['trans_month'] = df['trans_date'].dt.month\n",
        "    df['trans_day'] = df['trans_date'].dt.day\n",
        "    df['trans_weekday'] = df['trans_date'].dt.weekday\n",
        "    df['is_weekend'] = df['trans_weekday'].isin([5, 6]).astype(int)\n",
        "\n",
        "    # Transaction time\n",
        "    df['trans_time'] = pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.hour * 3600 + \\\n",
        "                       pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.minute * 60 + \\\n",
        "                       pd.to_datetime(df['trans_time'], format='%H:%M:%S').dt.second\n",
        "    # Average Spend\n",
        "    df['average_spend'] = df.groupby('cc_num')['amt'].transform('mean')\n",
        "\n",
        "    # Recency spend ratio\n",
        "    # Calculate rolling average spend for each cardholder\n",
        "    df['recency_spend'] = df.groupby('cc_num')['amt'].rolling(window=10, min_periods=1).mean().reset_index(0, drop=True)\n",
        "\n",
        "    # Calculate the recency spend ratio\n",
        "    df['recency_spend_ratio'] = df['amt'] / df['recency_spend']\n",
        "\n",
        "    # average spend at a given merchant\n",
        "    df['merchant_average_spend'] = df.groupby('merchant')['amt'].transform('mean')\n",
        "\n",
        "    # Calculate distance between user and merchant\n",
        "    df['distance'] = np.sqrt((df['lat'] - df['merch_lat'])**2 + (df['long'] - df['merch_long'])**2)\n",
        "\n",
        "    # Log-transform transaction amount\n",
        "    df['log_amt'] = np.log1p(df['amt'])\n",
        "\n",
        "    # Ensure no missing values in cc_num\n",
        "    df['cc_num'] = df['cc_num'].fillna(-1)\n",
        "    df['cc_num'] = df['cc_num'].fillna(-1)\n",
        "\n",
        "    # Calculate percentage change in spending for each cardholder\n",
        "    df['spend_percentage_change'] = df.groupby('cc_num')['amt'].pct_change().fillna(0)\n",
        "    df['spend_percentage_change'] = df.groupby('cc_num')['amt'].pct_change().fillna(0)\n",
        "\n",
        "    # Extract transaction day\n",
        "    df['trans_day'] = pd.to_datetime(df['trans_date']).dt.date\n",
        "\n",
        "    # Calculate velocity (number of transactions per day per cardholder)\n",
        "    df['velocity'] = df.groupby(['cc_num', 'trans_day'])['trans_num'].transform('count')\n",
        "\n",
        "    # Extract age from DOB\n",
        "    df['dob'] = pd.to_datetime(df['dob'])\n",
        "    df['age'] = (df['trans_date'] - df['dob']).dt.days // 365\n",
        "\n",
        "    #Encode categorical features\n",
        "    # df = pd.get_dummies(df, columns=['category', 'gender', 'state'], drop_first=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xDFJPgKLOJm9"
      },
      "outputs": [],
      "source": [
        "# Preprocess datasets\n",
        "train_df = preprocess_and_engineer_features(train_df)\n",
        "test_df = preprocess_and_engineer_features(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HCZ0vYxcOJm9"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "target = 'is_fraud'\n",
        "features = train_df.drop(columns=['id', 'trans_num', 'trans_date', 'recency_spend_ratio', 'average_spend','distance', 'spend_percentage_change','velocity', 'trans_time', 'dob', 'is_fraud'])\n",
        "test_features = test_df.drop(columns=['id', 'trans_num', 'trans_date','recency_spend_ratio', 'average_spend', 'distance', 'spend_percentage_change', 'velocity', 'trans_time', 'dob'])\n",
        "\n",
        "# Separate numerical and categorical columns\n",
        "numerical_cols = features.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_cols = features.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define preprocessing for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessors\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Preprocess the data\n",
        "X = features\n",
        "y = train_df[target]\n",
        "X_test = test_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg7ek0B8OJm9",
        "outputId": "3515ce57-51fc-435d-a7b9-4fc5291797d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1-Score: 0.9727\n",
            "Submission file 'submission.csv' created successfully.\n"
          ]
        }
      ],
      "source": [
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply the preprocessor\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_val = preprocessor.transform(X_val)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "model = XGBClassifier(n_estimators=1000, max_depth=20, scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]), learning_rate=0.3, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "print(f\"Validation F1-Score: {val_f1:.4f}\")\n",
        "\n",
        "# Predict on the test dataset\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "# Create the submission file\n",
        "submission = sample_submission.copy()\n",
        "submission['is_fraud'] = test_predictions\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file 'submission.csv' created successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}